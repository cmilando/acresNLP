) %>%
mutate(mod_sum = rowSums(across(flood_avg:fire_avg)))
ALL_TOWNS_list_sub <- setdiff(ALL_TOWNS_list,
unique(hazard_by_town$most_common_town))
hazard_by_town_blank <- data.frame(most_common_town = ALL_TOWNS_list_sub,
n = rep(0, length(ALL_TOWNS_list_sub)),
flood_avg = rep(NA, length(ALL_TOWNS_list_sub)),
storm_avg = rep(NA, length(ALL_TOWNS_list_sub)),
heat_avg = rep(NA, length(ALL_TOWNS_list_sub)),
air_pollution_avg = rep(NA, length(ALL_TOWNS_list_sub)),
indoor_air_avg = rep(NA, length(ALL_TOWNS_list_sub)),
chem_hazard_avg = rep(NA, length(ALL_TOWNS_list_sub)),
extreme_precip_avg = rep(NA, length(ALL_TOWNS_list_sub)),
fire_avg = rep(NA, length(ALL_TOWNS_list_sub)),
mod_sum = rep(0, length(ALL_TOWNS_list_sub)))
hazard_by_town$mod_sum
hazard_by_town <- rbind(hazard_by_town, hazard_by_town_blank)
hazard_name_map = c(
"air_pollution_avg" = 'Air Pollution',
'chem_hazard_avg' = 'Chemical Hazards',
'extreme_precip_avg' = 'Extreme Precipitation',
'fire_avg' = 'Fire',
'flood_avg' = 'Flood',
'heat_avg' = 'Heat',
'indoor_air_avg' = 'Indoor Air',
'storm_avg' = 'Storm'
)
capitalizeFirstLetter <- function(textVector) {
# Helper function to capitalize the first letter of a single string
singleCapitalize <- function(text) {
# Capitalize the first letter and leave the rest unchanged
paste0(toupper(substr(text, 1, 1)), substr(text, 2, nchar(text)))
}
# Apply the singleCapitalize function to each string in the vector
result <- sapply(textVector, singleCapitalize)
return(result)
}
# Example usage:
capitalizeFirstLetter(c("convert_text to camel_case", "another_example_here", "hello world"))
hazard_by_town
p1 <- hazard_by_town %>%
pivot_longer(cols = flood_avg:fire_avg) %>%
mutate(value = ifelse(value == 0, NA, value),
name_nice = hazard_name_map[name],
town_name_nice = paste0(capitalizeFirstLetter(most_common_town), " (", n, ")")) %>%
ggplot() +
geom_tile(aes(y = reorder(name_nice, value, na.rm = T),
x = town_name_nice,
fill = value),
color = 'white') +
scale_fill_binned(type = 'viridis',
name = 'Average proportion\nof per-document\nreferences',
limits = c(0, 100),
breaks = c(seq(20, 80, by = 20))) +
ylab('Hazard') + xlab('Town') +
scale_x_discrete(position = 'top') +
theme(axis.text.x = element_text(angle = 35, hjust = 0, size = 11),
axis.text.y = element_text(size = 14),
axis.title = element_text(size = 14))
p1
library(readr)
library(tidyverse)
library(ggforce)
library(janitor)
library(sf)
library(gtable)
library(grid)
library(gridExtra)
library(tigris)
library(leaflet)
### NOTES
# * STONEHAM HAS NO CLIMATE REPORTS
# * NOT A LOT OF ADJACENCY IN HAZARDS
#
# *
# * allison: for both hazards and outreach, for some towns the totals are < 100%
# * chad: expanding heat search terms and seeing if that changes things, manual search
# Needs:
# * Map
# * table of the heatmap with %s and n
# * REVERE AND LEXINGTON DON'T HAVE 100% in relevancy table
# * WHY ISNT HEAT MORE ** SEARCH TERMS
#
# * WHAT ABOUT THE COMMUNITY CONCERNS
# ----------------------------------------------------------------------------
#### create dataframe of hazard counts and proportions by town ####
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v8.tsv"))
problems(combined_table)
colnames(combined_table)
combined_table <- combined_table %>% clean_names()
combined_table$town_name <- gsub("url\\d+|\\d+|\\.json", "",
combined_table$file_name)
combined_table$town_name <- toupper(combined_table$town_name)
mystic_towns_list = c(
"Burlington",
"Lexington",
"Belmont",
"Watertown",
"Arlington",
"Winchester",
"Woburn",
"Reading",
"Stoneham",
"Medford",
"Somerville",
"Cambridge",
"Everett",
"Malden",
"Melrose",
"Wakefield",
"Chelsea",
"Revere",
"Winthrop",
"Wilmington"
)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
##
combined_table <- combined_table %>%
mutate(is_ACRES_town = (most_common_town %in% tolower(mystic_towns_list)),
is_INNER_CORE = (most_common_town %in% tolower(INNER_CORE$V1)),
is_MASS = (most_common_state == 'massachusetts'))
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
combined_table <- combined_table %>%
mutate(pass_checks2 = (
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
xx <- combined_table[, c('file_name', 'most_common_town','is_INNER_CORE',
'is_ACRES_town', 'duplicated', 'is_MASS',
'has_climate','has_community')] %>%
arrange(file_name)
write.table(xx, file = 'is_inner_core.txt',  sep = "|",
quote = F, row.names = F)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_2.csv"), header = T,
sep = "|")[,c('Manual.Check', "file_name")]
head(mancx)
dim(combined_table)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
write_tsv(combined_table, 'combined_table_v8.tsv')
source("01_make_passchecks.R")
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
# ----------------------------------------------------------------------------
# get the 1000s and add "include"
unique(combined_table$file_name)
# ----------------------------------------------------------------------------
# get the 1000s and add "include"
x1 <- combined_table$file_name
x2 <- gsub("^url", "", x1)
x2
x2 <- gsub("*^url", "", x1)
x2
x2 <- gsub("*^url", "", x1, perl = T)
x2 <- gsub("^url", "", x1, perl = T)
x2
x2 <- gsub(".*url", "", x1)
x2
x3 <- gsub(".json", "", x2)
x3
x4 <- as.numeric(x3)
which(x4 > 1000)
library(readr)
library(tidyverse)
library(ggforce)
library(janitor)
library(sf)
library(gtable)
library(grid)
library(gridExtra)
library(tigris)
library(leaflet)
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v8.tsv"))
problems(combined_table)
colnames(combined_table)
combined_table <- combined_table %>% clean_names()
combined_table$town_name <- gsub("url\\d+|\\d+|\\.json", "",
combined_table$file_name)
combined_table$town_name <- toupper(combined_table$town_name)
View(combined_table)
mystic_towns_list = c(
"Burlington",
"Lexington",
"Belmont",
"Watertown",
"Arlington",
"Winchester",
"Woburn",
"Reading",
"Stoneham",
"Medford",
"Somerville",
"Cambridge",
"Everett",
"Malden",
"Melrose",
"Wakefield",
"Chelsea",
"Revere",
"Winthrop",
"Wilmington"
)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
##
combined_table <- combined_table %>%
mutate(is_ACRES_town = (most_common_town %in% tolower(mystic_towns_list)),
is_INNER_CORE = (most_common_town %in% tolower(INNER_CORE$V1)),
is_MASS = (most_common_state == 'massachusetts'))
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
combined_table <- combined_table %>%
mutate(pass_checks2 = (
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
xx <- combined_table[, c('file_name', 'most_common_town','is_INNER_CORE',
'is_ACRES_town', 'duplicated', 'is_MASS',
'has_climate','has_community')] %>%
arrange(file_name)
write.table(xx, file = 'is_inner_core.txt',  sep = "|",
quote = F, row.names = F)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_2.csv"), header = T,
sep = "|")[,c('Manual.Check', "file_name")]
head(mancx)
dim(combined_table)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
write_tsv(combined_table, 'combined_table_v8.tsv')
source("01_make_passchecks.R")
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
# ----------------------------------------------------------------------------
# get the 1000s and add "include"
x1 <- combined_table$file_name
x2 <- gsub(".*url", "", x1)
x3 <- gsub(".json", "", x2)
x4 <- as.numeric(x3)
which(x4 > 1000)
which(combined_table$file_name == 'malden63.pdf')
which(combined_table$file_name == 'malden63.json')
which(combined_table$file_name == 'maldenurl63.json')
r1 <- which(combined_table$file_name == 'maldenurl63.json')
combined_table[r1, ]
glimpse(combined_table[r1, ])
View(combined_table)
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
r1
# burlington 85
r1 <- which(combined_table$file_name == 'burlington85.json')
r1
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# reading 60
r1 <- which(combined_table$file_name == 'readingurl60.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# wilmington
r1 <- which(combined_table$file_name == 'wilmingtonurl47.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# waltham 82
r1 <- which(combined_table$file_name == 'walthamurl82.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
pass_checks <- combined_table %>%
filter(pass_checks3) %>%
group_by(most_common_town) %>% tally()
sum(pass_checks$n)
write_tsv(pass_checks, 'pass_checks_0220.tsv')
write_tsv(combined_table, 'combined_table_v8_final.tsv')
### make flowchart
nrow(combined_table)
### make flowchart
nrow(combined_table)
table(combined_table$duplicated)
table(combined_table %>%
filter(duplicated == F) %>% select(is_MASS))
table(combined_table$duplicated)
table(combined_table %>%
filter(duplicated == F) %>% select(is_MASS))
table(combined_table %>%
filter(duplicated == F, is_MASS == T) %>%
select(is_INNER_CORE))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T) %>%
select(has_climate))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1) %>%
select(has_community))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1, has_community == 1) %>%
select(Manual.Check.11.19))
# Install and load the required libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
# Set your Census API key (get one from https://api.census.gov/data/key_signup.html)
census_api_key("840a4b0cd2dce4a1c58a9375612945f9f14ac5d9")
# Download data for all US places
VARS <- c("B01003_001","B19013_001")  #,"B08303_001","B08301_001"),
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = VARS,
year = 2020,
survey = "acs5"
)
dim(places_data)
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
head(INNER_CORE)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME, names_from = variable,
values_from = estimate)
head(INNER_CORE_df)
dim(INNER_CORE_df)
head(INNER_CORE_df)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME,
names_from = variable,
values_from = estimate)
o
tail(o)
View(o)
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
head(INNER_CORE)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
dim(INNER_CORE_df)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME,
names_from = variable,
values_from = estimate)
write.table(o, quote = F, sep = "|", row.names = F, file = 'TOWNS_CENSUS.txt')
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
head(INNER_CORE)
write.table(unique(INNER_CORE$V1),file =  "COMBINED_TOWNS.txt")
write.table(unique(INNER_CORE$V1),file =  "COMBINED_TOWNS.txt",
quote = F, row.names = F)
# Install and load the required libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
# Set your Census API key (get one from https://api.census.gov/data/key_signup.html)
census_api_key("840a4b0cd2dce4a1c58a9375612945f9f14ac5d9")
# Download data for all US places
VARS <- c("B01003_001","B19013_001")  #,"B08303_001","B08301_001"),
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = VARS,
year = 2020,
survey = "acs5"
)
dim(places_data)
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
dim(INNER_CORE_df)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME,
names_from = variable,
values_from = estimate)
write.table(o, quote = F, sep = "|", row.names = F, file = 'TOWNS_CENSUS.txt')
library(tidyverse)
xx <- read_tsv("combined_table_v8_final.tsv")
head(xx)
xx <- read_tsv("pass_checks_0220.tsv")
head(xx)
view(xx)
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
source("01_make_passchecks.R")
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# reading 60
r1 <- which(combined_table$file_name == 'readingurl60.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# wilmington
r1 <- which(combined_table$file_name == 'wilmingtonurl47.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# waltham 82
r1 <- which(combined_table$file_name == 'walthamurl82.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$pass_checks3[386]
glimpse(combind_table[386,])
glimpse(combined_table[386,])
View(combined_table)
combined_table %>% filter(most_common_town == 'winchester')
data.frame(combined_table %>% filter(most_common_town == 'winchester'))
which(combined_table$most_common_town == 'district')
which(combined_table$most_common_state == 'district')
