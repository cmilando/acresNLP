'After randomization,
the change in
P(high exposure &
low income)', low = muted("blue"), high = muted("red")) +
ylab("P(high exposure & low income)") +
xlab("P(high exposure)") +
#coord_cartesian(expand = F) +
theme_classic2() +
theme(strip.background = element_blank())
GRID_CELLS <- 20
inputs <- expand_grid(P_low_income = (0:10)/10,
P_high_exposure = (0:GRID_CELLS)/GRID_CELLS,
P_joint = (0:GRID_CELLS)/GRID_CELLS)
output <- sapply(1:nrow(inputs), function(i) get_pdiff(inputs$P_low_income[i],
inputs$P_high_exposure[i],
inputs$P_joint[i]))
final_df <- cbind(inputs, output)
custom_labeller <- function(variable, value) {
paste("P(low income) = ", value)
}
# plot for a slice
ggplot(final_df %>% filter(!(P_low_income %in% c(0, 1)))) +
geom_tile(aes(x = P_high_exposure,
y = P_joint,
fill = output)) +
facet_rep_wrap(~P_low_income, labeller = custom_labeller) +
#
# geom_segment(data = final_df %>%
#                filter(!(P_low_income %in% c(0, 1))) %>%
#                distinct(P_low_income, P_high_exposure),
#                mapping = aes(x = P_low_income,xend = P_low_income,
#                y = -Inf, yend = Inf), linetype = '41',
#              color = 'grey') +
# #
# geom_segment(data = final_df %>%
#                filter(!(P_low_income %in% c(0, 1))) %>%
#                distinct(P_low_income, P_high_exposure),
#              mapping = aes(y = P_low_income,yend = P_low_income,
#                            x = -Inf, xend = Inf), linetype = '41',
#              color = 'grey') +
#
geom_segment(data = final_df %>%
filter(!(P_low_income %in% c(0, 1))) %>%
distinct(P_low_income, P_high_exposure),
mapping = aes(y = P_low_income, yend = 1,
x = P_low_income, xend = 1),
linetype = '11',
color = 'grey') +
#
geom_segment(data = final_df %>%
filter(!(P_low_income %in% c(0, 1))) %>%
distinct(P_low_income, P_high_exposure),
mapping = aes(y = P_low_income, yend = P_low_income,
x = -Inf, xend = P_low_income),
linetype = '11',
color = 'grey') +
#
scale_fill_gradient2(name =
'After randomization,
the change in
P(high exposure &
low income)', low = muted("blue"), high = muted("red")) +
ylab("P(high exposure & low income)") +
xlab("P(high exposure)") +
#coord_cartesian(expand = F) +
theme_classic2() +
theme(strip.background = element_blank())
library(readr)
library(tidyverse)
library(ggforce)
library(janitor)
library(sf)
library(gtable)
library(grid)
library(gridExtra)
library(tigris)
library(leaflet)
### NOTES
# * STONEHAM HAS NO CLIMATE REPORTS
# * NOT A LOT OF ADJACENCY IN HAZARDS
#
# *
# * allison: for both hazards and outreach, for some towns the totals are < 100%
# * chad: expanding heat search terms and seeing if that changes things, manual search
# Needs:
# * Map
# * table of the heatmap with %s and n
# * REVERE AND LEXINGTON DON'T HAVE 100% in relevancy table
# * WHY ISNT HEAT MORE ** SEARCH TERMS
#
# * WHAT ABOUT THE COMMUNITY CONCERNS
# ----------------------------------------------------------------------------
#### create dataframe of hazard counts and proportions by town ####
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
# my_dir <- "C:/Users/ncesare/Downloads/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v7.tsv"))
colnames(combined_table)
combined_table <- combined_table %>% clean_names()
combined_table$town_name <- gsub("url\\d+|\\d+|\\.json", "",
combined_table$file_name)
combined_table$town_name <- toupper(combined_table$town_name)
#View(combined_table)
mystic_towns_list = c("Burlington", "Lexington", "Belmont", "Watertown",
"Arlington", "Winchester", "Woburn", "Reading",
"Stoneham", "Medford", "Somerville", "Cambridge",
"Boston", "Everett", "Malden", "Melrose",
"Wakefield", "Chelsea", "Revere", "Winthrop", "Wilmington",
"Newton", "Brookline", "Quincy", "Saugus", "Lynn",
"Needham", "Milton", "Waltham")
##
combined_table <- combined_table %>%
mutate(is_ACRES_town = (most_common_town %in% tolower(mystic_towns_list)),
is_MASS = (most_common_state == 'massachusetts'))
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
# write_tsv(combined_table[, c('file_name', 'duplicated')] %>%
#             arrange(file_name), 'duplicated.tsv')
combined_table <- combined_table %>%
mutate(pass_checks2 = (
duplicated == F &
is_ACRES_town == T &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
write_tsv(combined_table, 'combined_table_v7.tsv')
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_2.csv"), header = T,
sep = "|")[,c('Manual.Check', "file_name")]
head(mancx)
dim(combined_table)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
write_tsv(combined_table, 'combined_table_v7.tsv')
library(readr)
library(tidyverse)
library(ggforce)
library(janitor)
library(sf)
library(gtable)
library(grid)
library(gridExtra)
library(tigris)
library(leaflet)
### NOTES
# * STONEHAM HAS NO CLIMATE REPORTS
# * NOT A LOT OF ADJACENCY IN HAZARDS
#
# *
# * allison: for both hazards and outreach, for some towns the totals are < 100%
# * chad: expanding heat search terms and seeing if that changes things, manual search
# Needs:
# * Map
# * table of the heatmap with %s and n
# * REVERE AND LEXINGTON DON'T HAVE 100% in relevancy table
# * WHY ISNT HEAT MORE ** SEARCH TERMS
#
# * WHAT ABOUT THE COMMUNITY CONCERNS
# ----------------------------------------------------------------------------
#### create dataframe of hazard counts and proportions by town ####
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
# my_dir <- "C:/Users/ncesare/Downloads/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v7.tsv"))
colnames(combined_table)
combined_table <- combined_table %>% clean_names()
combined_table$town_name <- gsub("url\\d+|\\d+|\\.json", "",
combined_table$file_name)
combined_table$town_name <- toupper(combined_table$town_name)
#View(combined_table)
mystic_towns_list = c("Burlington", "Lexington", "Belmont", "Watertown",
"Arlington", "Winchester", "Woburn", "Reading",
"Stoneham", "Medford", "Somerville", "Cambridge",
"Boston", "Everett", "Malden", "Melrose",
"Wakefield", "Chelsea", "Revere", "Winthrop", "Wilmington",
"Newton", "Brookline", "Quincy", "Saugus", "Lynn",
"Needham", "Milton", "Waltham")
##
combined_table <- combined_table %>%
mutate(is_ACRES_town = (most_common_town %in% tolower(mystic_towns_list)),
is_MASS = (most_common_state == 'massachusetts'))
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
# write_tsv(combined_table[, c('file_name', 'duplicated')] %>%
#             arrange(file_name), 'duplicated.tsv')
combined_table <- combined_table %>%
mutate(pass_checks2 = (
duplicated == F &
is_ACRES_town == T &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
##
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_2.csv"), header = T,
sep = "|")[,c('Manual.Check', "file_name")]
head(mancx)
dim(combined_table)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
write_tsv(combined_table, 'combined_table_v7.tsv')
combined_table
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
source("01_make_passchecks.R")
source("01_make_passchecks.R")
problems()
problems(combined_table)
combined_table
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
is_ACRES_town == T &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
pass_checks <- combined_table %>%
filter(pass_checks3) %>%
group_by(most_common_town) %>% tally()
sum(pass_checks$n)
pass_checks
write_tsv(pass_checks, 'pass_checks_1202.tsv')
write_tsv(combined_table, 'combined_table_v7_final.tsv')
### make flowchart
table(combined_table$duplicated)
table(combined_table %>% filter(duplicated == F) %>% select(is_MASS))
table(combined_table %>%
filter(duplicated == F, is_MASS == T) %>%
select(is_ACRES_town))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_ACRES_town == T) %>%
select(has_climate))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_ACRES_town == T,
has_climate == 1) %>%
select(has_community))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_ACRES_town == T,
has_climate == 1, has_community == 1) %>%
select(Manual.Check))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_ACRES_town == T,
has_climate == 1, has_community == 1) %>%
select(Manual.Check.11.19))
Manual.Check
# ----------------------------------------------------------------------------
#90% or over for all towns
combined_table_relevant <- combined_table %>%
filter(pass_checks3)
dim(combined_table_relevant)
dim(combined_table)
dim(combined_table_relevant)
mystic_towns_list <- tolower(c("Burlington", "Lexington", "Belmont", "Watertown",
"Arlington", "Winchester", "Woburn", "Reading",
"Stoneham", "Medford", "Somerville", "Cambridge",
"Boston", "Charlestown", "Everett", "Malden", "Melrose",
"Wakefield", "Chelsea", "Revere", "Winthrop", "Wilmington"))
mystic_towns_list <- tolower(c("Burlington", "Lexington", "Belmont", "Watertown",
"Arlington", "Winchester", "Woburn", "Reading",
"Stoneham", "Medford", "Somerville", "Cambridge",
"Boston", "Charlestown", "Everett", "Malden", "Melrose",
"Wakefield", "Chelsea", "Revere", "Winthrop", "Wilmington",
"Newton", "Brookline", "Quincy", "Saugus", "Lynn",
"Needham", "Milton", "Waltham"))
hazard_by_town <- combined_table_relevant %>%
group_by(most_common_town) %>%
summarize(n = n(),
flood_avg = mean(flood_percent),
storm_avg = mean(storm_percent),
heat_avg = mean(heat_percent),
air_pollution_avg = mean(air_pollution_percent),
indoor_air_avg = mean(indoor_air_quality_percent),
chem_hazard_avg = mean(chemical_hazards_percent),
extreme_precip_avg = mean(extreme_precipitation_percent),
fire_avg = mean(fire_percent)
) %>%
mutate(mod_sum = rowSums(across(flood_avg:fire_avg)))
mystic_towns_list_sub <- setdiff(mystic_towns_list,  unique(hazard_by_town$most_common_town))
hazard_by_town_blank <- data.frame(most_common_town = mystic_towns_list_sub,
n = rep(0, length(mystic_towns_list_sub)),
flood_avg = rep(NA, length(mystic_towns_list_sub)),
storm_avg = rep(NA, length(mystic_towns_list_sub)),
heat_avg = rep(NA, length(mystic_towns_list_sub)),
air_pollution_avg = rep(NA, length(mystic_towns_list_sub)),
indoor_air_avg = rep(NA, length(mystic_towns_list_sub)),
chem_hazard_avg = rep(NA, length(mystic_towns_list_sub)),
extreme_precip_avg = rep(NA, length(mystic_towns_list_sub)),
fire_avg = rep(NA, length(mystic_towns_list_sub)),
mod_sum = rep(0, length(mystic_towns_list_sub)))
hazard_by_town$mod_sum
hazard_by_town <- rbind(hazard_by_town, hazard_by_town_blank)
hazard_name_map = c(
"air_pollution_avg" = 'Air Pollution',
'chem_hazard_avg' = 'Chemical Hazards',
'extreme_precip_avg' = 'Extreme Precipitation',
'fire_avg' = 'Fire',
'flood_avg' = 'Flood',
'heat_avg' = 'Heat',
'indoor_air_avg' = 'Indoor Air',
'storm_avg' = 'Storm'
)
capitalizeFirstLetter <- function(textVector) {
# Helper function to capitalize the first letter of a single string
singleCapitalize <- function(text) {
# Capitalize the first letter and leave the rest unchanged
paste0(toupper(substr(text, 1, 1)), substr(text, 2, nchar(text)))
}
# Apply the singleCapitalize function to each string in the vector
result <- sapply(textVector, singleCapitalize)
return(result)
}
# Example usage:
capitalizeFirstLetter(c("convert_text to camel_case", "another_example_here", "hello world"))
hazard_by_town
p1 <- hazard_by_town %>%
pivot_longer(cols = flood_avg:fire_avg) %>%
mutate(value = ifelse(value == 0, NA, value),
name_nice = hazard_name_map[name],
town_name_nice = paste0(capitalizeFirstLetter(most_common_town), " (", n, ")")) %>%
ggplot() +
geom_tile(aes(y = reorder(name_nice, value, na.rm = T),
x = town_name_nice,
fill = value),
color = 'white') +
scale_fill_binned(type = 'viridis',
name = 'Average proportion\nof per-document\nreferences',
limits = c(0, 100),
breaks = c(seq(20, 80, by = 20))) +
ylab('Hazard') + xlab('Town') +
scale_x_discrete(position = 'top') +
theme(axis.text.x = element_text(angle = 35, hjust = 0, size = 11),
axis.text.y = element_text(size = 14),
axis.title = element_text(size = 14))
p1
# Install and load the required libraries
library(tidycensus)
library(dplyr)
# Set your Census API key (get one from https://api.census.gov/data/key_signup.html)
census_api_key("840a4b0cd2dce4a1c58a9375612945f9f14ac5d9")
# Download data for all US places
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = c("B19013_001","B08303_001","B08301_001"),
year = 2020,
survey = "acs5"
)
dim(places_data)
# View the unique place names
place_names <- places_data %>%
select(NAME) %>%
distinct()
# Preview the place names
length(place_names[[1]])
ll <- strsplit(place_names[[1]], ",", fixed = T)
length(ll[[1]])
which(sapply(ll, length) > 2)
ll
ll <- do.call(rbind, ll)
ll2 <- strsplit(ll[, 1], " ", fixed = T)
ll3 <- sapply(ll2, function(l) l[1])
#
ll3
ll
head(places_data)
# Download data for all US places
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = c("B01003_001","B19013_001","B08303_001","B08301_001"),
year = 2020,
survey = "acs5"
)
dim(places_data)
head(places_data)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
i = 1
this_sub <- which(grepl(INNER_CORE[i], places_data$NAME))
this_sub
INNER_CORE[i]
this_sub <- which(grepl(INNER_CORE[i, 1], places_data$NAME))
this_sub
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(INNER_CORE[i, 1], places_data$NAME))
stopifnot(length(this_sub) == 4)
}
i
this_sub$NAME
places_data[this_sub,]
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == 4)
places_data[this_sub,]
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == 4)
places_data[this_sub,]
}
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == 4)
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
# Install and load the required libraries
library(tidycensus)
library(dplyr)
INNER_CORE_df
library(dplyr)
INNER_CORE_df %>% pivot_wider(id_cols = NAME, names_from = variable,
values_from = estimate)
library(tidyverse)
INNER_CORE_df %>% pivot_wider(id_cols = NAME, names_from = variable,
values_from = estimate)
# Download data for all US places
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = c("B01003_001","B19013_001"), #,"B08303_001","B08301_001"),
year = 2020,
survey = "acs5"
)
dim(places_data)
head(places_data)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == 4)
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
INNER_CORE_df %>% pivot_wider(id_cols = NAME, names_from = variable,
values_from = estimate)
i
# Download data for all US places
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = c("B01003_001","B19013_001"), #,"B08303_001","B08301_001"),
year = 2020,
survey = "acs5"
)
dim(places_data)
head(places_data)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
# Download data for all US places
VARS <- c("B01003_001","B19013_001")  #,"B08303_001","B08301_001"),
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = VARS,
year = 2020,
survey = "acs5"
)
dim(places_data)
head(places_data)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
INNER_CORE_df %>% pivot_wider(id_cols = NAME, names_from = variable,
values_from = estimate)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME, names_from = variable,
values_from = estimate)
write.table(o, quote = F, sep = "|", row.names = F, file = 'TOWNS_CENSUS.txt')
