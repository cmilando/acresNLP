# burlington 85
r1 <- which(combined_table$file_name == 'burlington85.json')
r1
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# reading 60
r1 <- which(combined_table$file_name == 'readingurl60.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# wilmington
r1 <- which(combined_table$file_name == 'wilmingtonurl47.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# waltham 82
r1 <- which(combined_table$file_name == 'walthamurl82.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
pass_checks <- combined_table %>%
filter(pass_checks3) %>%
group_by(most_common_town) %>% tally()
sum(pass_checks$n)
write_tsv(pass_checks, 'pass_checks_0220.tsv')
write_tsv(combined_table, 'combined_table_v8_final.tsv')
### make flowchart
nrow(combined_table)
### make flowchart
nrow(combined_table)
table(combined_table$duplicated)
table(combined_table %>%
filter(duplicated == F) %>% select(is_MASS))
table(combined_table$duplicated)
table(combined_table %>%
filter(duplicated == F) %>% select(is_MASS))
table(combined_table %>%
filter(duplicated == F, is_MASS == T) %>%
select(is_INNER_CORE))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T) %>%
select(has_climate))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1) %>%
select(has_community))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1, has_community == 1) %>%
select(Manual.Check.11.19))
# Install and load the required libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
# Set your Census API key (get one from https://api.census.gov/data/key_signup.html)
census_api_key("840a4b0cd2dce4a1c58a9375612945f9f14ac5d9")
# Download data for all US places
VARS <- c("B01003_001","B19013_001")  #,"B08303_001","B08301_001"),
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = VARS,
year = 2020,
survey = "acs5"
)
dim(places_data)
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
head(INNER_CORE)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME, names_from = variable,
values_from = estimate)
head(INNER_CORE_df)
dim(INNER_CORE_df)
head(INNER_CORE_df)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME,
names_from = variable,
values_from = estimate)
o
tail(o)
View(o)
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
head(INNER_CORE)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
dim(INNER_CORE_df)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME,
names_from = variable,
values_from = estimate)
write.table(o, quote = F, sep = "|", row.names = F, file = 'TOWNS_CENSUS.txt')
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
head(INNER_CORE)
write.table(unique(INNER_CORE$V1),file =  "COMBINED_TOWNS.txt")
write.table(unique(INNER_CORE$V1),file =  "COMBINED_TOWNS.txt",
quote = F, row.names = F)
# Install and load the required libraries
library(tidycensus)
library(tidyverse)
library(dplyr)
# Set your Census API key (get one from https://api.census.gov/data/key_signup.html)
census_api_key("840a4b0cd2dce4a1c58a9375612945f9f14ac5d9")
# Download data for all US places
VARS <- c("B01003_001","B19013_001")  #,"B08303_001","B08301_001"),
places_data <- get_acs(
geography = "place",
state = 'MA',
variables = VARS,
year = 2020,
survey = "acs5"
)
dim(places_data)
INNER_CORE <- read.table("COMBINED_TOWNS.txt", header = F)
INNER_CORE_data <- vector("list", nrow(INNER_CORE))
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
for(i in 1:nrow(INNER_CORE)) {
# i = 1
this_sub <- which(grepl(paste0(INNER_CORE[i, 1], " "), places_data$NAME))
stopifnot(length(this_sub) == length(VARS))
INNER_CORE_data[[i]] <- places_data[this_sub,]
}
INNER_CORE_df <- do.call(rbind, INNER_CORE_data)
dim(INNER_CORE_df)
o <-INNER_CORE_df %>% pivot_wider(id_cols = NAME,
names_from = variable,
values_from = estimate)
write.table(o, quote = F, sep = "|", row.names = F, file = 'TOWNS_CENSUS.txt')
library(tidyverse)
xx <- read_tsv("combined_table_v8_final.tsv")
head(xx)
xx <- read_tsv("pass_checks_0220.tsv")
head(xx)
view(xx)
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
source("01_make_passchecks.R")
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# reading 60
r1 <- which(combined_table$file_name == 'readingurl60.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# wilmington
r1 <- which(combined_table$file_name == 'wilmingtonurl47.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# waltham 82
r1 <- which(combined_table$file_name == 'walthamurl82.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$pass_checks3[386]
glimpse(combind_table[386,])
glimpse(combined_table[386,])
View(combined_table)
combined_table %>% filter(most_common_town == 'winchester')
data.frame(combined_table %>% filter(most_common_town == 'winchester'))
which(combined_table$most_common_town == 'district')
which(combined_table$most_common_state == 'district')
library(readr)
library(tidyverse)
library(ggforce)
library(janitor)
library(sf)
library(gtable)
library(grid)
library(gridExtra)
library(tigris)
library(leaflet)
### NOTES
# * STONEHAM HAS NO CLIMATE REPORTS
# * NOT A LOT OF ADJACENCY IN HAZARDS
#
# *
# * allison: for both hazards and outreach, for some towns the totals are < 100%
# * chad: expanding heat search terms and seeing if that changes things, manual search
# Needs:
# * Map
# * table of the heatmap with %s and n
# * REVERE AND LEXINGTON DON'T HAVE 100% in relevancy table
# * WHY ISNT HEAT MORE ** SEARCH TERMS
#
# * WHAT ABOUT THE COMMUNITY CONCERNS
# ----------------------------------------------------------------------------
#### create dataframe of hazard counts and proportions by town ####
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
# my_dir <- "C:/Users/ncesare/Downloads/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v8.tsv"))
problems(combined_table)
colnames(combined_table)
combined_table <- combined_table %>% clean_names()
combined_table$town_name <- gsub("url\\d+|\\d+|\\.json", "",
combined_table$file_name)
combined_table$town_name <- toupper(combined_table$town_name)
#View(combined_table)
mystic_towns_list = c(
"Burlington",
"Lexington",
"Belmont",
"Watertown",
"Arlington",
"Winchester",
"Woburn",
"Reading",
"Stoneham",
"Medford",
"Somerville",
"Cambridge",
"Everett",
"Malden",
"Melrose",
"Wakefield",
"Chelsea",
"Revere",
"Winthrop",
"Wilmington"
)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
##
combined_table <- combined_table %>%
mutate(is_ACRES_town = (most_common_town %in% tolower(mystic_towns_list)),
is_INNER_CORE = (most_common_town %in% tolower(INNER_CORE$V1)),
is_MASS = (most_common_state == 'massachusetts'))
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
# write_tsv(combined_table[, c('file_name', 'duplicated')] %>%
#             arrange(file_name), 'duplicated.tsv')
combined_table <- combined_table %>%
mutate(pass_checks2 = (
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
xx <- combined_table[, c('file_name', 'most_common_town','is_INNER_CORE',
'is_ACRES_town', 'duplicated', 'is_MASS',
'has_climate','has_community')] %>%
arrange(file_name)
write.table(xx, file = 'is_inner_core.txt',  sep = "|",
quote = F, row.names = F)
##
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_2.csv"), header = T,
sep = "|")[,c('Manual.Check', "file_name")]
head(mancx)
dim(combined_table)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
write_tsv(combined_table, 'combined_table_v8.tsv')
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
source("01_make_passchecks.R")
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
r1
combined_table[820,]
combined_table[821,]
combined_table$url[820]
View(combined_table)
# and any with url that starts with https://www.mass.gov/doc/
which(grepl("https://www.mass.gov/doc/", combined_table$url))
combined_table[81]
combined_table[81,]
# and any with url that starts with https://www.mass.gov/doc/
r1 <- which(grepl("https://www.mass.gov/doc/", combined_table$url))
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
glimpse(combined_table[386,])
glimpse(combined_table[820,])
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# reading 60
r1 <- which(combined_table$file_name == 'readingurl60.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# wilmington
r1 <- which(combined_table$file_name == 'wilmingtonurl47.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
# waltham 82
r1 <- which(combined_table$file_name == 'walthamurl82.json')
r1
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
View(combined_table)
# and any with url that starts with https://www.mass.gov/doc/
r1 <- which(grepl("https://www.mass.gov/doc/", combined_table$url))
combined_table$Manual.Check.11.19[r1] <- 'INCLUDE'
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
glimpse(combined_table[820,])
pass_checks <- combined_table %>%
filter(pass_checks3) %>%
group_by(most_common_town) %>% tally()
sum(pass_checks$n)
write_tsv(pass_checks, 'pass_checks_0220.tsv')
write_tsv(combined_table, 'combined_table_v8_final.tsv')
### make flowchart
nrow(combined_table)
table(combined_table$duplicated)
table(combined_table %>%
filter(duplicated == F) %>% select(is_MASS))
table(combined_table %>%
filter(duplicated == F, is_MASS == T) %>%
select(is_INNER_CORE))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T) %>%
select(has_climate))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1) %>%
select(has_community))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1, has_community == 1) %>%
select(Manual.Check.11.19))
source("01_make_passchecks.R")
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
r1
combined_table$pass_checks3[r1] <- 'INCLUDE'
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$pass_checks3[r1] <- 'INCLUDE'
# reading 60
r1 <- which(combined_table$file_name == 'readingurl60.json')
r1
combined_table$pass_checks3[r1] <- 'INCLUDE'
# wilmington
r1 <- which(combined_table$file_name == 'wilmingtonurl47.json')
r1
combined_table$pass_checks3[r1] <- 'INCLUDE'
# waltham 82
r1 <- which(combined_table$file_name == 'walthamurl82.json')
r1
combined_table$pass_checks3[r1] <- 'INCLUDE'
# and any with url that starts with https://www.mass.gov/doc/
r1 <- which(grepl("https://www.mass.gov/doc/", combined_table$url))
combined_table$pass_checks3[r1] <- 'INCLUDE'
##
pass_checks <- combined_table %>%
filter(pass_checks3) %>%
group_by(most_common_town) %>% tally()
sum(pass_checks$n)
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
# malden 63
r1 <- which(combined_table$file_name == 'maldenurl63.json')
r1
combined_table$pass_checks3[r1] <- TRUE
# burlington 85
r1 <- which(combined_table$file_name == 'burlingtonurl85.json')
r1
combined_table$pass_checks3[r1] <- TRUE
# reading 60
r1 <- which(combined_table$file_name == 'readingurl60.json')
r1
combined_table$pass_checks3[r1] <- TRUE
# wilmington
r1 <- which(combined_table$file_name == 'wilmingtonurl47.json')
r1
combined_table$pass_checks3[r1] <- TRUE
# waltham 82
r1 <- which(combined_table$file_name == 'walthamurl82.json')
r1
combined_table$pass_checks3[r1] <- TRUE
# and any with url that starts with https://www.mass.gov/doc/
r1 <- which(grepl("https://www.mass.gov/doc/", combined_table$url))
combined_table$pass_checks3[r1] <- TRUE
##
pass_checks <- combined_table %>%
filter(pass_checks3) %>%
group_by(most_common_town) %>% tally()
sum(pass_checks$n)
write_tsv(pass_checks, 'pass_checks_0220.tsv')
write_tsv(combined_table, 'combined_table_v8_final.tsv')
which(combined_table$most_common_town == None)
which(combined_table$most_common_town == 'None')
combined_table[245,]
combined_table$url[245]
