#   beta_TEMP = 1
#
#   cases_VL = beta_state * state + beta_TEMP * temp
# }
return(cases_VL)
}
#
df$myV = NA
for(i in 1:nrow(df)) {
df$myV[i] <- my_Vcalc_1(df$state[i], df$temp[i], i)
}
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
geom_line(aes(x = day, y = myV)) +
facet_grid(~state)
# for each of these, write a function that describes how the betas change
# for each level
my_Vcalc_1 <- function(state, temp, i) {
stopifnot(length(state) == 1)
stopifnot(length(temp) == 1)
beta_state = 100
beta_TEMP = if(temp > 20, 10 * state, 10)
#
df$myV = NA
for(i in 1:nrow(df)) {
df$myV[i] <- my_Vcalc_1(df$state[i], df$temp[i], i)
}
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
geom_line(aes(x = day, y = myV)) +
facet_grid(~state)
# for each of these, write a function that describes how the betas change
# for each level
my_Vcalc_1 <- function(state, temp, i) {
stopifnot(length(state) == 1)
stopifnot(length(temp) == 1)
beta_state = 100
beta_TEMP = if(temp == 30, 10 * state, 10)
#
df$myV = NA
for(i in 1:nrow(df)) {
df$myV[i] <- my_Vcalc_1(df$state[i], df$temp[i], i)
}
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
geom_line(aes(x = day, y = myV)) +
facet_grid(~state)
df$temp <- ifelse(df$day > 150 & df$day < 250, 30, 20)
library(tidyverse)
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
facet_grid(~state)
# for each of these, write a function that describes how the betas change
# for each level
my_Vcalc_1 <- function(state, temp, i) {
stopifnot(length(state) == 1)
stopifnot(length(temp) == 1)
beta_TEMP = if(temp == 30, 1 * state, 1)
#
df$myV = NA
for(i in 1:nrow(df)) {
df$myV[i] <- my_Vcalc_1(df$state[i], df$temp[i], i)
}
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
geom_line(aes(x = day, y = myV)) +
facet_grid(~state)
df[367,]
# for each of these, write a function that describes how the betas change
# for each level
my_Vcalc_1 <- function(state, temp, i) {
stopifnot(length(state) == 1)
stopifnot(length(temp) == 1)
beta_TEMP = if(temp == 30, 1 * state, 1)
#
df$myV = NA
for(i in 1:nrow(df)) {
df$myV[i] <- my_Vcalc_1(df$state[i], df$temp[i], i)
}
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
geom_line(aes(x = day, y = myV)) +
facet_grid(~state)
# for each of these, write a function that describes how the betas change
# for each level
my_Vcalc_1 <- function(state, temp, i) {
stopifnot(length(state) == 1)
stopifnot(length(temp) == 1)
beta_TEMP = ifelse(temp == 30, 1 * state, 1)
set.seed(i)
epsilon = rnorm(1)
epsilon = 0
alpha = 100
cases_VL =  alpha + beta_TEMP * temp + epsilon
# if(temp < 30) {
#
# }
#
#
# if(state == 1 & temp == 30) {
#   beta_SVI = 2
#   beta_TEMP = 4
#   cases_VL = beta_state * state + beta_TEMP * temp
# }
#
# else{
#   beta_state = 1
#   beta_TEMP = 1
#
#   cases_VL = beta_state * state + beta_TEMP * temp
# }
return(cases_VL)
}
#
df$myV = NA
for(i in 1:nrow(df)) {
df$myV[i] <- my_Vcalc_1(df$state[i], df$temp[i], i)
}
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
geom_line(aes(x = day, y = myV)) +
facet_grid(~state)
library(randomForest)
install.packages("randomForest")
library(randomForest)
set.seed(1)
bag.VL = randomForest(myV ∼ temp*factor(state) + factor(state),
bag.VL = randomForest(myV ∼ .,
library(randomForest)
set.seed(1)
bag.VL = randomForest(myV ∼ .,
bag.VL = randomForest(myV∼.,
bag.VL = randomForest(myV∼., data=df, mtry=2,importance =TRUE)
?randomForest
set.seed(71)
iris.rf <- randomForest(Species ~ ., data=iris, importance=TRUE,
proximity=TRUE)
head(df)
bag.VL = randomForest(myV ∼ ., data=df,importance =TRUE)
bag.VL <- randomForest(myV~., data=df,importance =TRUE)
bag.VL <- randomForest(myV~ temp*factor(state) + state,
data=df,importance =TRUE)
bag.VL
summary(bag.VL)
getTree(bag.VL)
plot(bag.VL)
bag.VL <- cForest(myV~ temp*factor(state) + state,
data=df,importance =TRUE)
bag.VL <- cforest(myV~ temp*factor(state) + state,
data=df,importance =TRUE)
library(cforest)
install.packages(cforest)
install.packages("party")
library(party)
set.seed(1)
head(df)
bag.VL <- cforest(myV~ temp*factor(state) + state,
data=df,importance =TRUE)
bag.VL <- cforest(myV~ temp*factor(state) + state)
bag.VL <- cforest(myV~ temp*factor(state) + state, data = df)
warnings()
bag.VL <- cforest(myV~ temp*factor(state) + state, data = df,
mtry = 2)
ggplot(df) +
geom_line(aes(x = day, y = temp)) +
geom_line(aes(x = day, y = myV)) +
facet_grid(~state)
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v7.tsv"))
library(readr)
library(tidyverse)
library(ggforce)
library(janitor)
library(sf)
library(gtable)
library(grid)
library(gridExtra)
library(tigris)
library(leaflet)
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v7.tsv"))
problems(combined_table)
colnames(combined_table)
combined_table <- combined_table %>% clean_names()
combined_table$town_name <- gsub("url\\d+|\\d+|\\.json", "",
combined_table$file_name)
combined_table$town_name <- toupper(combined_table$town_name)
mystic_towns_list = c(
"Burlington",
"Lexington",
"Belmont",
"Watertown",
"Arlington",
"Winchester",
"Woburn",
"Reading",
"Stoneham",
"Medford",
"Somerville",
"Cambridge",
"Everett",
"Malden",
"Melrose",
"Wakefield",
"Chelsea",
"Revere",
"Winthrop",
"Wilmington"
)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
##
combined_table <- combined_table %>%
mutate(is_ACRES_town = (most_common_town %in% tolower(mystic_towns_list)),
is_INNER_CORE = (most_common_town %in% tolower(INNER_CORE$V1)),
is_MASS = (most_common_state == 'massachusetts'))
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
combined_table <- combined_table %>%
mutate(pass_checks2 = (
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
xx <- combined_table[, c('file_name', 'most_common_town','is_INNER_CORE',
'is_ACRES_town', 'duplicated', 'is_MASS',
'has_climate','has_community')] %>%
arrange(file_name)
write.table(xx, file = 'is_inner_core.txt',  sep = "|",
quote = F, row.names = F)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_2.csv"), header = T,
sep = "|")[,c('Manual.Check', "file_name")]
head(mancx)
dim(combined_table)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
write_tsv(combined_table, 'combined_table_v7.tsv')
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
data.frame(head(combined_table))
combined_table <- combined_table %>%
mutate(pass_checks3 = (
Manual.Check.11.19 %in% c('INCLUDE') &
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
pass_checks <- combined_table %>%
filter(pass_checks3) %>%
group_by(most_common_town) %>% tally()
sum(pass_checks$n)
write_tsv(pass_checks, 'pass_checks_1202.tsv')
write_tsv(combined_table, 'combined_table_v7_final.tsv')
### make flowchart
nrow(combined_table)
table(combined_table$duplicated)
table(combined_table %>%
filter(duplicated == F) %>% select(is_MASS))
table(combined_table %>%
filter(duplicated == F, is_MASS == T) %>%
select(is_INNER_CORE))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T) %>%
select(has_climate))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1) %>%
select(has_community))
table(combined_table %>%
filter(duplicated == F, is_MASS == T, is_INNER_CORE == T,
has_climate == 1, has_community == 1) %>%
select(Manual.Check.11.19))
# ----------------------------------------------------------------------------
#90% or over for all towns
combined_table_relevant <- combined_table %>%
filter(pass_checks3)
dim(combined_table_relevant)
dim(combined_table)
dim(combined_table_relevant)
# Charlestown removed
ALL_TOWNS <- read.table("COMBINED_TOWNS.txt", header = F)
head(ALL_TOWNS)
# Charlestown removed
ALL_TOWNS <- read.table("COMBINED_TOWNS.txt", header = F)
head(ALL_TOWNS)
ALL_TOWNS_list <- tolower(ALL_TOWNS$V1)
hazard_by_town <- combined_table_relevant %>%
group_by(most_common_town) %>%
summarize(n = n(),
flood_avg = mean(flood_percent),
storm_avg = mean(storm_percent),
heat_avg = mean(heat_percent),
air_pollution_avg = mean(air_pollution_percent),
indoor_air_avg = mean(indoor_air_quality_percent),
chem_hazard_avg = mean(chemical_hazards_percent),
extreme_precip_avg = mean(extreme_precipitation_percent),
fire_avg = mean(fire_percent)
) %>%
mutate(mod_sum = rowSums(across(flood_avg:fire_avg)))
ALL_TOWNS_list_sub <- setdiff(ALL_TOWNS_list,
unique(hazard_by_town$most_common_town))
hazard_by_town_blank <- data.frame(most_common_town = ALL_TOWNS_list_sub,
n = rep(0, length(ALL_TOWNS_list_sub)),
flood_avg = rep(NA, length(ALL_TOWNS_list_sub)),
storm_avg = rep(NA, length(ALL_TOWNS_list_sub)),
heat_avg = rep(NA, length(ALL_TOWNS_list_sub)),
air_pollution_avg = rep(NA, length(ALL_TOWNS_list_sub)),
indoor_air_avg = rep(NA, length(ALL_TOWNS_list_sub)),
chem_hazard_avg = rep(NA, length(ALL_TOWNS_list_sub)),
extreme_precip_avg = rep(NA, length(ALL_TOWNS_list_sub)),
fire_avg = rep(NA, length(ALL_TOWNS_list_sub)),
mod_sum = rep(0, length(ALL_TOWNS_list_sub)))
hazard_by_town$mod_sum
hazard_by_town <- rbind(hazard_by_town, hazard_by_town_blank)
hazard_name_map = c(
"air_pollution_avg" = 'Air Pollution',
'chem_hazard_avg' = 'Chemical Hazards',
'extreme_precip_avg' = 'Extreme Precipitation',
'fire_avg' = 'Fire',
'flood_avg' = 'Flood',
'heat_avg' = 'Heat',
'indoor_air_avg' = 'Indoor Air',
'storm_avg' = 'Storm'
)
capitalizeFirstLetter <- function(textVector) {
# Helper function to capitalize the first letter of a single string
singleCapitalize <- function(text) {
# Capitalize the first letter and leave the rest unchanged
paste0(toupper(substr(text, 1, 1)), substr(text, 2, nchar(text)))
}
# Apply the singleCapitalize function to each string in the vector
result <- sapply(textVector, singleCapitalize)
return(result)
}
# Example usage:
capitalizeFirstLetter(c("convert_text to camel_case", "another_example_here", "hello world"))
hazard_by_town
p1 <- hazard_by_town %>%
pivot_longer(cols = flood_avg:fire_avg) %>%
mutate(value = ifelse(value == 0, NA, value),
name_nice = hazard_name_map[name],
town_name_nice = paste0(capitalizeFirstLetter(most_common_town), " (", n, ")")) %>%
ggplot() +
geom_tile(aes(y = reorder(name_nice, value, na.rm = T),
x = town_name_nice,
fill = value),
color = 'white') +
scale_fill_binned(type = 'viridis',
name = 'Average proportion\nof per-document\nreferences',
limits = c(0, 100),
breaks = c(seq(20, 80, by = 20))) +
ylab('Hazard') + xlab('Town') +
scale_x_discrete(position = 'top') +
theme(axis.text.x = element_text(angle = 35, hjust = 0, size = 11),
axis.text.y = element_text(size = 14),
axis.title = element_text(size = 14))
p1
library(readr)
library(tidyverse)
library(ggforce)
library(janitor)
library(sf)
library(gtable)
library(grid)
library(gridExtra)
library(tigris)
library(leaflet)
### NOTES
# * STONEHAM HAS NO CLIMATE REPORTS
# * NOT A LOT OF ADJACENCY IN HAZARDS
#
# *
# * allison: for both hazards and outreach, for some towns the totals are < 100%
# * chad: expanding heat search terms and seeing if that changes things, manual search
# Needs:
# * Map
# * table of the heatmap with %s and n
# * REVERE AND LEXINGTON DON'T HAVE 100% in relevancy table
# * WHY ISNT HEAT MORE ** SEARCH TERMS
#
# * WHAT ABOUT THE COMMUNITY CONCERNS
# ----------------------------------------------------------------------------
#### create dataframe of hazard counts and proportions by town ####
#adjust based on your computer
#my_dir <- "/Users/alliej/Library/CloudStorage/OneDrive-BostonUniversity/ACRES NLP/acresNLP/"
my_dir <- "/Users/cwm/Documents/GitHub/acresNLP/"
create_df <- function(filename){
data <- read_delim(filename)
return(data)
}
#combined table missing all arlington/belmont and some chelsea/everett
combined_table <- create_df(paste0(my_dir, "combined_output_v8.tsv"))
problems(combined_table)
colnames(combined_table)
combined_table <- combined_table %>% clean_names()
combined_table$town_name <- gsub("url\\d+|\\d+|\\.json", "",
combined_table$file_name)
combined_table$town_name <- toupper(combined_table$town_name)
mystic_towns_list = c(
"Burlington",
"Lexington",
"Belmont",
"Watertown",
"Arlington",
"Winchester",
"Woburn",
"Reading",
"Stoneham",
"Medford",
"Somerville",
"Cambridge",
"Everett",
"Malden",
"Melrose",
"Wakefield",
"Chelsea",
"Revere",
"Winthrop",
"Wilmington"
)
INNER_CORE <- read.table("INNER_CORE.txt", header = F)
head(INNER_CORE)
##
combined_table <- combined_table %>%
mutate(is_ACRES_town = (most_common_town %in% tolower(mystic_towns_list)),
is_INNER_CORE = (most_common_town %in% tolower(INNER_CORE$V1)),
is_MASS = (most_common_state == 'massachusetts'))
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
# duplicated
combined_table$duplicated = duplicated(combined_table$first100words)
combined_table <- combined_table %>%
mutate(pass_checks2 = (
duplicated == F &
(is_INNER_CORE == T | is_ACRES_town == T) &
is_MASS == T &
has_climate == 1 &
has_community == 1
))
xx <- combined_table[, c('file_name', 'most_common_town','is_INNER_CORE',
'is_ACRES_town', 'duplicated', 'is_MASS',
'has_climate','has_community')] %>%
arrange(file_name)
write.table(xx, file = 'is_inner_core.txt',  sep = "|",
quote = F, row.names = F)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_2.csv"), header = T,
sep = "|")[,c('Manual.Check', "file_name")]
head(mancx)
dim(combined_table)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
write_tsv(combined_table, 'combined_table_v8.tsv')
source("01_make_passchecks.R")
dim(combined_table)
table(combined_table$most_common_town, combined_table$pass_checks2)
table(combined_table$pass_checks2)
mancx <- read.csv(paste0(my_dir, "manual_checks_1202.csv"), header = T,
sep = "|")
head(mancx)
combined_table <- combined_table %>% left_join(mancx)
dim(combined_table)
data.frame(head(combined_table))
# ----------------------------------------------------------------------------
# get the 1000s and add "include"
unique(combined_table$file_name)
# ----------------------------------------------------------------------------
# get the 1000s and add "include"
x1 <- combined_table$file_name
x2 <- gsub("^url", "", x1)
x2
x2 <- gsub("*^url", "", x1)
x2
x2 <- gsub("*^url", "", x1, perl = T)
x2 <- gsub("^url", "", x1, perl = T)
x2
x2 <- gsub(".*url", "", x1)
x2
x3 <- gsub(".json", "", x2)
x3
x4 <- as.numeric(x3)
which(x4 > 1000)
